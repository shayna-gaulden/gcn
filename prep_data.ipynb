{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict object;\n",
    "ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "print(x.shape)                  (140, 1433)\n",
    "print(tx.shape)                 (1000, 1433)\n",
    "print(allx.shape)               (1708, 1433)\n",
    "print(y.shape)                  (140, 7)\n",
    "print(ty.shape)                 (1000, 7)\n",
    "print(ally.shape)               (1708, 7)\n",
    "print(len(graph.keys()))        2708\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'goemotions/goemotions_1.csv') # load in the data\n",
    "# going to use a sample of the dataset first since it is so large\n",
    "# adj will be saved as the graph\n",
    "# x corresponds to document nodes xall will include the word nodes\n",
    "# feature vectors will just be the identity matrix for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = data.columns[9:] # save the name of the emotions columns\n",
    "yval = data.iloc[:,[1,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]].groupby('id').sum()\n",
    "for c in yval.columns:\n",
    "    yval.loc[yval[c]!=0,c] = 1 # union of all anotators labeling\n",
    "\n",
    "xval = data.iloc[:,[0,1]].drop_duplicates().set_index('id') # remove duplicates\n",
    "data = pd.concat([xval,yval],join=\"inner\",axis=1) # join the text and labels on the same ID\n",
    "data = data.drop(data.index[data[emotions].sum(axis=1) == 0]) # remove rows with no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>1511</td>\n",
       "      <td>3169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>754</td>\n",
       "      <td>1779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>786</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>1408</td>\n",
       "      <td>2634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>1868</td>\n",
       "      <td>3504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>602</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>809</td>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>960</td>\n",
       "      <td>1868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>391</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>923</td>\n",
       "      <td>1630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>1161</td>\n",
       "      <td>2256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>547</td>\n",
       "      <td>1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>244</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>647</td>\n",
       "      <td>1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>306</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>898</td>\n",
       "      <td>2034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>74</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>799</td>\n",
       "      <td>1516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>684</td>\n",
       "      <td>1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>205</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>882</td>\n",
       "      <td>1659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>157</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>966</td>\n",
       "      <td>1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>181</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>269</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>686</td>\n",
       "      <td>1242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>533</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>4994</td>\n",
       "      <td>10216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0      1\n",
       "admiration      1511   3169\n",
       "amusement        754   1779\n",
       "anger            786   1484\n",
       "annoyance       1408   2634\n",
       "approval        1868   3504\n",
       "caring           602   1203\n",
       "confusion        809   1390\n",
       "curiosity        960   1868\n",
       "desire           391    736\n",
       "disappointment   923   1630\n",
       "disapproval     1161   2256\n",
       "disgust          547   1020\n",
       "embarrassment    244    512\n",
       "excitement       647   1083\n",
       "fear             306    619\n",
       "gratitude        898   2034\n",
       "grief             74    142\n",
       "joy              799   1516\n",
       "love             684   1508\n",
       "nervousness      205    358\n",
       "optimism         882   1659\n",
       "pride            157    265\n",
       "realization      966   1726\n",
       "relief           181    252\n",
       "remorse          269    480\n",
       "sadness          686   1242\n",
       "surprise         533   1067\n",
       "neutral         4994  10216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame([ty.sum(axis=0),yall.sum(axis=0)]).transpose() # looking at the counts of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kimkiamco/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/kimkiamco/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kimkiamco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/kimkiamco/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import emoji\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_custom(text):\n",
    "    t1 = re.sub(r'https?://\\S+|www\\.\\S+', '', text)     # remove URLS\n",
    "    t2 = re.sub(\"@[A-Za-z0-9_]+\",\"\", t1)                # remove user mentions\n",
    "    t3 = re.sub(\"#\",\"\",t2)                              # remove '#' but keep the hashtag\n",
    "    t4 = re.sub(r'(.)\\1+', r'\\1\\1\\1', t3)               # Reduce the length of repeated characters\n",
    "    t5 = emoji.demojize(t4)                             # Replace emojis\n",
    "\n",
    "    # # Replace slang TODO\n",
    "    t6 = t5\n",
    "\n",
    "    t7 = re.sub(r'[0-9]+', '', t6)                      # Remove numbers 0-9\n",
    "    t8 = contractions.fix(t7)                           # Expanding contractions'\n",
    "    t9 = re.sub(r\"[^\\P{P}!?]+\",\"\",t8)                   # remove puntuation except for ! and ?\n",
    "    \n",
    "    # TODO remove abbrieviations\n",
    "    t10 = t9\n",
    "\n",
    "    t11 = \" \".join(t10.split())                         # Remove extra whitespace\n",
    "\n",
    "    word_list = nltk.word_tokenize(t11)\n",
    "    # remove stop words\n",
    "    filtered_sentence = [w for w in word_list if not w.lower() in stop_words]\n",
    "    # Lemmatization\n",
    "    t12 = ' '.join([lemmatizer.lemmatize(w) for w in filtered_sentence]).lower()\n",
    "    return t12\n",
    "\n",
    "xval = data.text.apply(preprocess_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "eew5j0j                                            game hurt\n",
       "ed2mah1                                    right care fuck !\n",
       "eeibobj                                     man love redddit\n",
       "eda6yn6                             name nowhere near falcon\n",
       "eespn2i    right ? considering important document know da...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import sparse\n",
    "vectorizer = CountVectorizer()\n",
    "xval = vectorizer.fit_transform(xval) # this returns a document term frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = sparse.csr_matrix(np.zeros([xval.shape[0],xval.shape[0]])) # create  matrix of zeros\n",
    "z2 = sparse.csr_matrix(np.zeros([xval.shape[1],xval.shape[1]])) # create  matrix of zeros\n",
    "graph = sparse.vstack([sparse.hstack([z1,xval]),sparse.hstack([xval.transpose(),z2])]) # W = [0 x; xt 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "print(x.shape)                  (140, 1433)     (only train docs, parameter)\n",
    "print(tx.shape)                 (1000, 1433)    (only test docs, paramet)\n",
    "print(allx.shape)               (1708, 1433)    (num train docs + num words, paramet)\n",
    "print(y.shape)                  (140, 7)        (train docs, 28)\n",
    "print(ty.shape)                 (1000, 7)       (test docs, 28)\n",
    "print(ally.shape)               (1708, 7)       (num train docs + num words, 28)\n",
    "print(len(graph.keys()))        2708            (num docs + num words)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68636\n",
      "25619\n"
     ]
    }
   ],
   "source": [
    "# convert graph to dictionary\n",
    "ndocs_nwords = graph.shape[0]\n",
    "print(ndocs_nwords)\n",
    "nwords = ndocs_nwords - data.shape[0]\n",
    "print(nwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30112, 100)\n",
      "(12905, 100)\n",
      "(55731, 100)\n",
      "(30112, 28)\n",
      "(12905, 28)\n",
      "(55731, 28)\n",
      "(68636, 68636)\n"
     ]
    }
   ],
   "source": [
    "n_features = 100\n",
    "\n",
    "zy = np.zeros([nwords,len(emotions)])\n",
    "testidx = int(data.shape[0]*0.3) # test index will just be the first 0.3% of the dataset\n",
    "ty = data[emotions][0:testidx]\n",
    "tx = np.identity(testidx)[:,:n_features]\n",
    "y = data[emotions][testidx:]\n",
    "x = np.identity(y.shape[0])[:,:n_features]\n",
    "ally = np.vstack([np.array(y),zy])\n",
    "allx = np.identity(yall.shape[0])[:,:n_features]\n",
    "\n",
    "print(x.shape)\n",
    "print(tx.shape)\n",
    "print(allx.shape)\n",
    "print(y.shape)\n",
    "print(ty.shape)\n",
    "print(ally.shape)\n",
    "print(graph.shape)\n",
    "\n",
    "test_index = list(range(testidx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._arrays.csr_array"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type(sparse.csr_array(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save variables\n",
    "with open('gcn_v2/data/ind.goemo.test.index', 'wb') as f:\n",
    "    pickle.dump(test_index, f)\n",
    "with open('gcn_v2/data/ind.goemo.x', 'wb') as f:\n",
    "    pickle.dump(x, f)\n",
    "with open('gcn_v2/data/ind.goemo.tx', 'wb') as f:\n",
    "    pickle.dump(tx, f)\n",
    "with open('gcn_v2/data/ind.goemo.allx', 'wb') as f:\n",
    "    pickle.dump(allx, f)\n",
    "with open('gcn_v2/data/ind.goemo.y', 'wb') as f:\n",
    "    pickle.dump(y, f)\n",
    "with open('gcn_v2/data/ind.goemo.ty', 'wb') as f:\n",
    "    pickle.dump(ty, f)\n",
    "with open('gcn_v2/data/ind.goemo.ally', 'wb') as f:\n",
    "    pickle.dump(ally, f)\n",
    "with open('gcn_v2/data/ind.goemo.graph', 'wb') as f:\n",
    "    pickle.dump(sparse.csr_array(graph), f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
